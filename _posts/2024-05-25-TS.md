---
title: "Tree Search algorithms for solving games (MiniMax, Alpha-Beta and MCTS) üå≥"
date: 2024-05-25
permalink: /posts/2024/05/RandomForests/
excerpt: "In this post, I share how tree searches work and I share my code for using them in any game you want."
---

# <img src="/images/TS/TS1.jpeg" width="900" height="200">
# <img src="/images/DIR.png" width="300" height="70" style="font-size: 15px;">

In this post, I share how tree searches work and I share my code for using them in any game you want.



## üì∞ General Presentation of Tree Searches



In game theory, tree searches are algorithms that try to find the best move in a given state of the game, and with limited knowledge on the environment. These tree searches can be used for playing Connect4, Chess, Go, but also for more sophisticated tasks, like training a computer how to find the flaws of a website.



The hypothesis used in this post will be:

- The tree search is about finding the best move in 1 versus 1 game. The game environment is deterministic.
- Both players know the rules of the game:
    - *Knowledge of the game dynamics:* They know which actions lead to which states and they know all the actions available in any state.
    - *Knowledge of the game winning conditions*: They know that for certains states, they may be some winning actions for player 1 or player 2, which end the game, and they know these states and actions.
- Players know some ‚Äúheuristics‚Äù. ‚ÄúHeuristics‚Äù are functions that allow to discriminate between different actions even though the rules do not give any way. ‚ÄúHeuristics‚Äù are built by approximating the game environment and using some of its features to tell if a position is closer to winning or losing. Different algorithms require different types of heuristics.



## üî¨How do MiniMax and Alpha-Beta tree searches work ?



### Minimax tree search

The algorithm works by creating a tree of possible game states, starting from the current position. It then navigates down the tree to the terminal states - the states where the game is either won, lost, or drawn.

<p>A fixed maximum depth \( d \) is given, an all leaves are evaluated: high value if it is favorable for the ('maximizing') player 1, and a low value if it is favorable for the ('minimizing') player 2. The evaluation function is an heuristic reward function. For example, an heuristic reward function for chess can be constructed by counting the values of the remaining pieces of each side on a given position (state). The heuristic reward function have to return the highest possible number when the state is a winning state for the player 1, and the lowest possible number if it is a winning state for the player 2.</p>

Once the values of the leaves states are determined, the algorithm propagates these values up the tree. For each node, if it's the 'maximizer's' turn, it takes the maximum value of its child nodes. If it's the 'minimizer's' turn, it takes the minimum value instead. This process continues until the value of the root is determined.

This root value is then used to decide the best move for the 'maximizing' player. The move that leads to the child node with the root value is chosen.

Here is the pseudo code for minimax.

# <img src="/images/TS/TS3.png" width="500" height="700">
### Alpha-Beta tree search

Alpha-Beta tree search (aka Alpha-Beta pruning) is a more efficient variant of the Minimax algorithm. It greatly improves the efficiency of the minimax algorithm by reducing the number of nodes that need to be examined.

<p>We start by fixing a maximum depth \( d \) and going down to the leaves, using a heuristic reward function like in Minimax. During the traversal, the algorithm prunes the branches that do not need to be searched because they cannot be played by rational players.</p>

<p>To do so, we have to maintain two values, \( \alpha \) and \( \beta \): \( \alpha \) represent the maximum lower bound of the possible reward achieved after \( d \) moves, \( \beta \) the minimum upper bound of the score possible. \( \alpha \) is the mimimum reward player 1 can expect and \( \beta \) is the equivalent for player 2.</p>

<p> These values are initialized to \( -\infty \) and \( +\infty \) respectively. \( \alpha \) and \( \beta \) are updated recursively:</p>

$$
\text{eval} = \text{alphabeta}(child, depth - 1, \alpha, \beta , \text{player 2}) \\ \alpha = \max(\alpha, \text{eval}) 
$$
**(for the player 1)**

$$\text{eval} = \text{alphabeta}(child, depth - 1, \alpha, \beta , \text{player 1}) \\ \beta = \min(\beta, \text{eval}) 
$$
**(for the player 2)**

<p> When the \( \alpha \) value becomes greater than or equal to the \( \beta \) value, this means that the node can not be reached by rational players, and so this branch is not explored recursively.</p>

For the algorithm to be the most efficient possible, it has to visit the best possible children of a node as soon as possible, so it requires an **heuristic sorting** of the actions.

For example an **heuristic sorting** for chess can be: visiting the ‚Äúcapture‚Äù actions first, and then the rest.

# <img src="/images/TS/TS4.png" width="500" height="900">

## üî¨How does Monte-Carlo tree search work ?


Monte Carlo Tree Search (MCTS) is a non-recursive search algorithm that uses random sampling as its primary form of exploration. Unlike Minimax and Alpha-Beta search, MCTS does not rely on an **heuristic reward** function. Instead, it performs a walk down the game tree, simulating games to the end and using the results to inform its search.

MCTS is composed of four subprocesses:

- Selection: select the best **path** according to the past experience, making the best choice for each play for each player at each level (while allowing a form of exploration, using a UCB correction term)
- Expansion: All the children of the ‚Äòend of the **path** are added to the tree and their number of visits and wins are initialized to zero.
- Simulation (aka play-out): Simulate a game starting from one of these children. The game can be ran randomly, or following a default policy provided.
- Backpropagation: Knowing the result of the simulation, the number of visits and wins are updated on each node of the **path.**

# <img src="/images/TS/TS2.png" width="770" height="300">

# <img src="/images/TS/TS5.png" width="600" height="500">



## üë®‚ÄçüíªMy implementation ([source code](https://github.com/Hadrien-Cr/Discover-Implement-Repeat/tree/main/Reinforcement_Learning/TreeSearch))



I followed the last pseudo codes for creating an Agent_Tree_Search class. Given a game environment, an agent can return the best move found by each search. The minimax search was not parallelized even thought it could be (unlike the other searches that are not parallelizable).

The environment game requires some methods, that correspond to the *knowledge of game dynamics* (the environment has to have a env.get_next_state(state,action) method and a env.get_available_actions(state,bit_player)) and the *knowledge of the winning conditions* (the env has to have a env.reward(s,a) that outputs 1 if the player 1 wins by playing action a in the state s (resp -1 for player 2) else 0).

Note that the heuristic function reward have to be scaled to the interval [-1;1] to fit the fact that the best possible reward is 1 and the worst possible reward is -1.

## Validation of the implementation

The MCTS algorithm is in the top ~10/230 of the ConnectX Kaggle competition https://www.kaggle.com/code/hadriencr/connectx-tree-searches-1250-elo which works with a 2-month rolling leaderboard.