---
title: "Nelder‚ÄìMead Algorithm üìâ"
date: 2024-03-30
permalink: /posts/2024/03/Nelder‚ÄìMead/
excerpt: "In this post, I showcase a full implementation of the Nelder-Mead algorithm."
---

# <img src="/images/NM/NM1.jpeg" width="900" height="200">
# <img src="/images/DIR.png" width="300" height="70" style="font-size: 15px;">

Read on Medium: [Nelder-Mead Algorithm](https://medium.com/@crassoushadrien/nelder-mead-algorithm-a9075162722e)



## üì∞ Quick presentation

The Nelder-Mead algorithm is a deterministic optimization method suitable for continuous multi-dimensional functions, requiring no knowledge of the function's derivative. The algorithm starts from a simplex in the input space (the Nelder-Mead simplex) and iteratively updates it by reflection, contraction, expansion, or shrinking to converge to an optimal point.

![visualization](/images/NM/NM2.gif)



## üî¨ Breaking down the algorithm

### Choice of the simplex

A [simplex](https://en.wikipedia.org/wiki/Simplex) in \( n \)-dimensions has \( (n+1) \) vertices.

<img src="/images/NM/NM3.png" alt="examples of regular simplexes in dimensions 1,2,3" width="477" height="259" class="jop-noMdConv">

### Rules for updating the simplex

Assuming \( x_1, x_2, x_3 \) are ordered by decreasing performance (e.g., minimizing a loss function \( f \)):

- **Reflection:** If \( x_r \) (reflected point from \( x_c \)) performs better than \( x_1 \) but not \( x_1 \), update by reflecting with a factor \( \alpha \).

  <img src="/images/NM/NM5.png" alt=" reflection" width="208" height="172" class="jop-noMdConv">

- **Expansion:** If \( x_r \) performs better than \( x_1 \), update by expanding with a factor \( \beta \).

  <img src="/images/NM/NM6.png" alt="expansion" width="200" height="200" class="jop-noMdConv">

- **Outside-contraction:** If \( x_r \) performs better than \( x_2 \), update by outside-contraction with a factor \( \rho \).

  <img src="/images/NM/NM7.png" alt="outside-contraction" width="140" height="140" class="jop-noMdConv">

- **Inside-contraction:** If \( x_r \) does not perform better than \( x_2 \), update by inside-contraction with a factor \( \rho \).

  <img src="/images/NM/NM8.png" alt="inside-contraction" width="140" height="140" class="jop-noMdConv">

- **Shrinkage:** If none of the above updates improve \( x_r \), shrink towards \( x_1 \) with a factor \( \delta \).

  <img src="/images/NM/NM9.png" alt="shrinkage" width="140" height="140" class="jop-noMdConv">



## Limitations of the algorithm

- The algorithm is sensitive to high noise compared to Batch Gradient Descent.
- Performance heavily depends on the choice of the initial simplex.
- Scaling issues of features can affect convergence speed.

<img src="/images/NM/NM10.png" alt="difference between non-normalized and normalized features" width="400" height="200" class="jop-noMdConv">

- The algorithm lacks adaptive learning rates, but adaptations have been proposed in literature.



## üë®‚Äçüíª My implementation [(Source code)](https://github.com/Hadrien-Cr/Discover-Implement-Repeat/tree/main/Optimization/NELDER_MEAD)

<p>For implementation, Numpy arrays were used to represent simplexes. Standard values for \( \alpha, \beta, \rho, \delta \) were employed. The algorithm cannot be vectorized due to dependent iterations.</p>

For implementation, Numpy arrays were used to represent simplexes. Standard values for \( \alpha, \beta, \rho, \delta \) were employed. The algorithm cannot be vectorized due to dependent iterations.



## üìã Benchmark

Comparison with scipy‚Äôs implementation using four different loss functions:

- Rosenbrock function
- Sphere function
- Rastrigin function
- Ackley function

### Benchmarking errors

Error in minimum value estimation compared for different \( \text{max iterations} \) and arbitrary simplex.

<img src="/images/NM/NM13.jpg" alt="benchmarking errors" width="400" height="200" class="jop-noMdConv">

### Benchmarking execution times

Execution times compared under the same conditions.

<img src="/images/NM/NM12.png" alt="benchmarking execution times" width="400" height="200" class="jop-noMdConv">

Scipy‚Äôs implementation appears faster on average by 10 to 50%.

---

The implementation demonstrates success!
