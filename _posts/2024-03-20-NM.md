---
title: "Nelder‚ÄìMead Algorithm üìâ"
date: 2024-03-30
permalink: /posts/2024/03/Nelder‚ÄìMead/
excerpt: "In this post, I showcase a full implementation of the Nelder-Mead algorithm."
---

# <img src="/images/NM/NM1.jpeg" width="900" height="200">
# <img src="/images/DIR.png" width="300" height="70" style="font-size: 15px;">

Read on Medium: [Nelder-Mead Algorithm](https://medium.com/@crassoushadrien/nelder-mead-algorithm-a9075162722e)



## üì∞ Quick presentation

The Nelder-Mead algorithm is a deterministic optimization method suitable for continuous multi-dimensional functions, requiring no knowledge of the function's derivative. The algorithm starts from a simplex in the input space (the Nelder-Mead simplex) and iteratively updates it by reflection, contraction, expansion, or shrinking to converge to an optimal point.

![visualization](/images/NM/NM2.gif)



## üî¨ Breaking down the algorithm

### Choice of the simplex

<p>A [simplex](https://en.wikipedia.org/wiki/Simplex) in \( n \)-dimensions has \( (n+1) \) vertices.</p>

<img src="/images/NM/NM3.png" alt="examples of regular simplexes in dimensions 1,2,3" width="477" height="259" class="jop-noMdConv">

### Rules for updating the simplex

<p>Assuming \( x_1, x_2, x_3 \) are ordered by decreasing performance (e.g., minimizing a loss function \( f \)):</p>

<p>Reflection: If \( x_r \) (reflected point from \( x_c \)) performs better than \( x_1 \) but not \( x_1 \), update by reflecting with a factor \( \alpha \).</p>

  <img src="/images/NM/NM5.png" alt=" reflection" width="208" height="172" class="jop-noMdConv">

<p>Expansion: If \( x_r \) performs better than \( x_1 \), update by expanding with a factor \( \beta \).</p>

  <img src="/images/NM/NM6.png" alt="expansion" width="200" height="200" class="jop-noMdConv">

<p>Outside-contraction: If \( x_r \) performs better than \( x_2 \), update by outside-contraction with a factor \( \rho \).</p>

  <img src="/images/NM/NM7.png" alt="outside-contraction" width="140" height="140" class="jop-noMdConv">

<p>Inside-contraction: If \( x_r \) does not perform better than \( x_2 \), update by inside-contraction with a factor \( \rho \).</p>

  <img src="/images/NM/NM8.png" alt="inside-contraction" width="140" height="140" class="jop-noMdConv">

<p>Shrinkage: If none of the above updates improve \( x_r \), shrink towards \( x_1 \) with a factor \( \delta \).</p>

  <img src="/images/NM/NM9.png" alt="shrinkage" width="140" height="140" class="jop-noMdConv">



## Limitations of the algorithm

- The algorithm is sensitive to high noise compared to Batch Gradient Descent.
- Performance heavily depends on the choice of the initial simplex.
- Scaling issues of features can affect convergence speed.

<img src="/images/NM/NM10.png" alt="difference between non-normalized and normalized features" width="400" height="200" class="jop-noMdConv">

- The algorithm lacks adaption in the update rules, so it sometimes get stuck in some cycles.
But adaptations have been proposed in literature [Implementing the Nelder-Mead simplex algorithm with adaptive parameters,Fuchang Gao ¬∑ Lixing Han](http://dx.doi.org/10.1007/s10589-010-9329-3) They have proposed:
$$
\alpha = 1, \beta = 1+\frac{2}{n}, \rho = 0.75-\frac{1}{2n}, \delta = 1+\frac{2}{n}
$$
with many improvements on benchmarcks functions, especially for high dimensional problems.


## üë®‚Äçüíª My implementation [(Source code)](https://github.com/Hadrien-Cr/Discover-Implement-Repeat/tree/main/Optimization/NELDER_MEAD)

<p>For implementation, Numpy arrays were used to represent simplexes. Standard values for \( \alpha, \beta, \rho, \delta \) were employed. The algorithm cannot be vectorized due to dependent iterations.</p>


## üìã Benchmark

Comparison with scipy‚Äôs implementation using four different loss functions:

- Rosenbrock function
- Sphere function
- Rastrigin function
- Ackley function

### Benchmarking errors

Error in minimum value estimation compared for different \( \text{max iterations} \) and arbitrary simplex.

<img src="/images/NM/NM13.jpg" alt="benchmarking errors" width="400" height="200" class="jop-noMdConv">

### Benchmarking execution times

Execution times compared under the same conditions.

<img src="/images/NM/NM12.png" alt="benchmarking execution times" width="400" height="200" class="jop-noMdConv">

Scipy‚Äôs implementation appears faster on average by 10 to 50%. 

---

The implementation demonstrates success!
