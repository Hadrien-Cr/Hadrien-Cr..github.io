---
title: "Nelder‚ÄìMead Algorithm üìâ"
date: 2024-03-30
permalink: /posts/2024/03/Nelder‚ÄìMead/
excerpt: "In this post, I showcase a full implementation of Nelder-Mead algorithm."
---

# <img src="/images/NM/NM1.jpeg" width="900" height="200">
# <img src="/images/DIR.png" width="300" height="70" style="font-size: 15px;">

Read in Medium: https://medium.com/@crassoushadrien/nelder-mead-algorithm-a9075162722e

* * *

## üì∞ Quick presentation

Nelder-Mead algorithm is a determistic optimization method. It is suitable for continuous multi-dimensional functions and it does not require to know anything about the derivative of the function.

The principle of the algorithm is to start from a shape in the input space *(the Nelder-Mead simplex)*, updating it (by reflecting it, contracting it, expanding it or shrinking it) such that after enough updates, the shape is a small zone containing the target optimizing point.


*![visualization](/images/NM/NM2.gif)visualization of the first steps of the algorithm*

* * *

## üî¨Breaking down the algorithm

### Choice of the shape

A [simplex](https://en.wikipedia.org/wiki/Simplex) is the generalization of the notion of a¬†triangle to dimensions greater than 2 (ie in dimension 3, tetraedrons are simplexes).

A $$n$$\-dimensional simplex has $$(n+1)$$ vertices.

<img src="/images/NM/NM3.png" alt="examples of regular simplexes in dimensions 1,2,3" width="477" height="259" class="jop-noMdConv">

examples of regular simplexes in dimensions 1,2,3

### Rules for updating the simplex

Lets assume $$x_1, x_2, x_3$$ are ordered by decreasing performance (compare to the objective, for example minimizing a loss function $$f$$ )

Then $$x_3$$ is the worst performing point. We want to avoid updating towards $$x_3$$and we want to know want is a good update to do.

Lets denote $$x_c$$ the centroid of the n other points.

<img src="/images/NM/NM4.png" alt="the initial simplex" width="168" height="172" class="jop-noMdConv">the initial simplex

1.  If the reflected point $$x_r$$ **($$x_3$$ reflected from $$x_c$$)** performs better than $$x_3$$ but not better than $$x_1$$, **the update is the following reflection of factor $$\alpha$$ (fixed)**

(standard value is $$\alpha=1$$)

<img src="/images/NM/NM5.png" alt=" reflection" width="208" height="172" class="jop-noMdConv"> reflection

2.  If the reflected point $$x_r$$ performs better than $$x_3$$ and better than $$x_1$$, **the update is an expansion of factor $$\beta$$ (fixed)**

(standard value is $$\beta=2$$)

<img src="/images\NM\NM6.png" alt="expansion" width="200" height="200"> expansion

- If $$x_r$$ performs better than $$x_2$$ *($$x_n$$ if the dimension is $$n$$)*, the **update is an outside-contraction of factor $$\rho$$ (fixed)**
    
    (standard value is $$\rho=0.5$$)
    
<img src="/images\NM\NM7.png" alt="outside-contraction" width="140" height="140"> outside-contraction
    
- else if $$x_r$$ does not performs better than $$x_2$$, the **update is an inside-contraction of factor $$\rho$$**
    

<img src="/images\NM\NM8.png" alt="outside-contraction" width="140" height="140"> inside-contraction

- If the last update was not an improvement on $$x_r$$, **then do this shrinkage towards x1x1x1 instead of a factor $$\delta$$ (fixed)**

(standard value is $$\delta = 0.5$$)

<img src="/images\NM\NM9.png" alt="shrinkage" width="140" height="140"> shrinkage

* * *

## Limitations of the algorithm:

- The biggest flaw of the algorithm is that it doesn‚Äôt implement any solution to high noise, unlike Batch Gradient Descent algorithm.
    
- The performance depends on the choice of the initial simplex.
    
- If the features are not scaled the same way (ie the loss function is sensitive to a small step in the x-axis but not in the y-axis because it is somehow upscaled), the algorithm would take longer to converge. It is a issue that is common in many optimizing methods.
    

<img src="/images\NM\NM10.png" alt="diff_NM" width="400" height="200"> 

difference between non normalized features an normalized features

- The algorith does not implemennt any kind of adaptative learning rate. One option could be to start with extremes values of **$$\alpha,\beta, \rho, \delta$$** and \*\*\*\*to \*\*\*\*scale them back to values that are more appropriate to a small domain. An article from Fuchang Gao and Lixing Han [(Implementing the Nelder-Mead simplex algorithm with adaptive parameters DOI: 10.1007/s10589-010-9329-3)](https://webpages.uidaho.edu/~fuchang/res/ANMS.pdf) implement this adaptation:
    
    $$
     \alpha=1, \beta = 1 +\frac2n,  \rho = 0.75 ‚àí \frac
    2n, \delta=1-\frac1n \\ \text{  (n being the index of the iteration)}
    $$
    
    This adaptation on the ‚Äúupdate magnitude‚Äù solves in some cases the search being stuck (cf the article).
    

* * *

## üë®‚ÄçüíªMy implementation [(Source code)](https://github.com/Hadrien-Cr/Discover-Implement-Repeat/tree/main/Optimization/NELDER_MEAD)

For the implementation, I used Numpy arrays for representing simplexes.

I used the standard values for **$$\alpha, \beta, \rho, \delta.$$**

There is no way to vectorize the algorithm because the iterations are not independent.

* * *

## üìã Benchmark

I wanted to compare my custom implementation with scipy‚Äôs implementation.

For Benchmarking, I picked 4 different loss functions from https://en.wikipedia.org/wiki/Test_functions_for_optimization :Rosenbrock function, Sphere function, Rastrigin function, and Ackley function.

### Benchmarking the errors

I made graphs that plot the error of the minimum value estimation, for my algorithm and for scipy implementation, for different values of the $$max \ iterations$$ cap, and for the same arbitrary simplex.

Note than none of the implementations succeed in finding the minimum of the [Ackley function](https://en.wikipedia.org/wiki/Ackley_function) and the [Rastrigin function](https://en.wikipedia.org/wiki/Rastrigin_function)

<img src="/images\NM\NM11.png" alt="bm1" width="400" height="200"> 

### Benchmarking the execution times

I also wanted to benchmarch the execution times. I used the same conditions as the last experiment. (There are some spikes, I think it is due to my machine.)

<img src="/images\NM\NM12.png" alt="bm2" width="400" height="200"> 

(the experiment is repeated 50 times)

Scipy‚Äôs implementation seems faster on average by 10 to 50 %.

* * *

The implementation is successful !

* * *